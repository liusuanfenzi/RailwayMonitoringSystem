#!/usr/bin/env python3
"""
åŸºäºçœŸå®è§†é¢‘çš„é“è½¨å…‰å½±å˜åŒ–ç­–ç•¥å®éªŒ
"""

from utils.video.video_utils import VideoReader
from models.background_subtractors.gmm_model_old1 import GMMBackgroundSubtractor
import cv2
import numpy as np
import time
import sys
import os
from pathlib import Path
import matplotlib.pyplot as plt

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class LightChangeExperiment:
    """å…‰å½±å˜åŒ–ç­–ç•¥å®éªŒ"""

    def __init__(self, video_path):
        self.video_path = video_path
        self.results = {}

    def analyze_light_changes(self, frame_sequence):
        """åˆ†æå¸§åºåˆ—ä¸­çš„äº®åº¦å˜åŒ–"""
        brightness_changes = []

        for frame in frame_sequence:
            if frame is not None:
                # è½¬æ¢ä¸ºç°åº¦å›¾è®¡ç®—å¹³å‡äº®åº¦
                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
                avg_brightness = np.mean(gray)
                brightness_changes.append(avg_brightness)

        return brightness_changes

    def detect_significant_light_changes(self, brightness_changes, threshold=10):
        """æ£€æµ‹æ˜¾è‘—çš„äº®åº¦å˜åŒ–"""
        changes = []
        for i in range(1, len(brightness_changes)):
            change = abs(brightness_changes[i] - brightness_changes[i-1])
            if change > threshold:
                changes.append(
                    (i, change, brightness_changes[i] - brightness_changes[i-1]))
        return changes

    def run_strategy_comparison(self, max_frames=700):
        """è¿è¡Œç­–ç•¥æ¯”è¾ƒå®éªŒ"""
        print("ğŸ¬ å¼€å§‹çœŸå®è§†é¢‘ç­–ç•¥æ¯”è¾ƒå®éªŒ...")

        if not Path(self.video_path).exists():
            print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {self.video_path}")
            return

        # ç­–ç•¥é…ç½®
        strategies = [
            {
                'name': 'KNN-equalization',
                'algorithm': 'KNN',
                'params': {
                    'history': 200,
                    'dist2_threshold': 400,
                    'use_histogram_equalization': True,
                    'use_median_blur': True,
                    'gaussian_kernel': (5, 5)
                },
                'roi': [(50, 200), (600, 900)]
            },
            {
                'name': 'KNN-sensitive',
                'algorithm': 'KNN',
                'params': {
                    'history': 100,
                    'dist2_threshold': 300,
                    'use_histogram_equalization': False,
                    'use_median_blur': False
                },
                'roi': [(50, 200), (600, 900)]
            },
            {
                'name': 'KNN-mix',
                'algorithm': 'KNN',
                'params': {
                    'history': 150,
                    'dist2_threshold': 350,
                    'use_histogram_equalization': False,
                    'use_median_blur': True,
                    'gaussian_kernel': (3, 3)
                },
                'roi': [(50, 200), (600, 900)]
            },
            {
                'name': 'MOG2-comparison',
                'algorithm': 'MOG2',
                'params': {
                    'history': 200,
                    'var_threshold': 16,
                    'use_histogram_equalization': False,
                    'use_median_blur': True
                },
                'roi': [(50, 200), (600, 900)]
            }
        ]

        reader = VideoReader(self.video_path)
        frames = []
        frame_count = 0

        # è¯»å–å¸§åºåˆ—
        print("ğŸ“¥ è¯»å–è§†é¢‘å¸§...")
        while frame_count < max_frames:
            frame = reader.read_frame()
            if frame is None:
                break
            frames.append(frame)
            frame_count += 1
            if frame_count % 50 == 0:
                print(f"   å·²è¯»å– {frame_count} å¸§")

        reader.release()

        if not frames:
            print("âŒ æ— æ³•è¯»å–è§†é¢‘å¸§")
            return

        # åˆ†æäº®åº¦å˜åŒ–
        print("ğŸ“Š åˆ†æè§†é¢‘äº®åº¦å˜åŒ–...")
        brightness_changes = self.analyze_light_changes(frames)
        significant_changes = self.detect_significant_light_changes(
            brightness_changes)

        print(f"   æ€»å¸§æ•°: {len(frames)}")
        print(f"   æ£€æµ‹åˆ°æ˜¾è‘—äº®åº¦å˜åŒ–: {len(significant_changes)} æ¬¡")

        # è¿è¡Œå„ç­–ç•¥
        for strategy in strategies:
            print(f"\nğŸ”§ æµ‹è¯•ç­–ç•¥: {strategy['name']}")

            try:
                detector = GMMBackgroundSubtractor(
                    algorithm=strategy['algorithm'],
                    **strategy['params']
                )
                detector.setup_track_roi(strategy['roi'])

                foreground_ratios = []
                processing_times = []

                for i, frame in enumerate(frames):
                    start_time = time.time()

                    result = detector.apply_with_roi_analysis(frame)

                    processing_time = time.time() - start_time
                    processing_times.append(processing_time)

                    foreground_ratio = result['full_frame']['foreground_ratio']
                    foreground_ratios.append(foreground_ratio)

                # è®¡ç®—ç­–ç•¥è¡¨ç°æŒ‡æ ‡
                avg_foreground = np.mean(foreground_ratios)
                std_foreground = np.std(foreground_ratios)
                avg_processing_time = np.mean(processing_times)

                # æ£€æµ‹äº®åº¦å˜åŒ–æœŸé—´çš„å“åº”
                light_change_responses = []
                for change_frame, change_amount, _ in significant_changes:
                    if change_frame < len(foreground_ratios):
                        light_change_responses.append(
                            foreground_ratios[change_frame])

                avg_light_response = np.mean(
                    light_change_responses) if light_change_responses else 0

                self.results[strategy['name']] = {
                    'avg_foreground': avg_foreground,
                    'std_foreground': std_foreground,
                    'avg_processing_time': avg_processing_time,
                    'avg_light_response': avg_light_response,
                    'foreground_ratios': foreground_ratios,
                    'processing_times': processing_times
                }

                print(f"   âœ… å¹³å‡å‰æ™¯æ¯”ä¾‹: {avg_foreground:.4f}")
                print(f"   âœ… å‰æ™¯ç¨³å®šæ€§: {std_foreground:.4f}")
                print(f"   âœ… å¹³å‡å¤„ç†æ—¶é—´: {avg_processing_time*1000:.2f}ms")
                print(f"   âœ… äº®åº¦å˜åŒ–å“åº”: {avg_light_response:.4f}")

            except Exception as e:
                print(f"   âŒ ç­–ç•¥æµ‹è¯•å¤±è´¥: {e}")

        return self.results, brightness_changes, significant_changes

    def visualize_results(self, results, brightness_changes, significant_changes):
        """å¯è§†åŒ–å®éªŒç»“æœ"""
        print("\nğŸ“ˆ ç”Ÿæˆå®éªŒç»“æœå¯è§†åŒ–...")

        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))

        # 1. äº®åº¦å˜åŒ–è¶‹åŠ¿
        ax1.plot(brightness_changes, label='avg_brightness',
                 color='blue', alpha=0.7)
        ax1.set_title('video_brightness_trend')
        ax1.set_xlabel('frame_index')
        ax1.set_ylabel('brightness')
        ax1.grid(True, alpha=0.3)

        # æ ‡è®°æ˜¾è‘—å˜åŒ–ç‚¹
        for change_frame, change_amount, direction in significant_changes:
            color = 'red' if direction > 0 else 'green'
            ax1.axvline(x=change_frame, color=color, alpha=0.5, linestyle='--')

        # 2. å„ç­–ç•¥å‰æ™¯æ¯”ä¾‹å¯¹æ¯”
        for strategy_name, result in results.items():
            ax2.plot(result['foreground_ratios'],
                     label=strategy_name, alpha=0.7)
        ax2.set_title('each_strategy_foreground_ratio')
        ax2.set_xlabel('frame_index')
        ax2.set_ylabel('foreground_ratio')
        ax2.legend()
        ax2.grid(True, alpha=0.3)

        # 3. æ€§èƒ½å¯¹æ¯”
        strategy_names = list(results.keys())
        processing_times = [
            results[name]['avg_processing_time'] * 1000 for name in strategy_names]
        bars = ax3.bar(strategy_names, processing_times, alpha=0.7)
        ax3.set_title('avg_processing_time')
        ax3.set_ylabel('processing_time (ms)')

        # åœ¨æŸ±çŠ¶å›¾ä¸Šæ·»åŠ æ•°å€¼
        for bar, time_val in zip(bars, processing_times):
            ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.1,
                     f'{time_val:.1f}ms', ha='center', va='bottom')

        # 4. ç»¼åˆè¡¨ç°é›·è¾¾å›¾
        metrics = ['detect_sensitivity', 'stability', 'processing_time', 'light_response']
        strategy_performance = {}

        for strategy_name, result in results.items():
            # å½’ä¸€åŒ–å„é¡¹æŒ‡æ ‡
            sensitivity = result['avg_foreground']  # å‰æ™¯æ¯”ä¾‹è¶Šé«˜ï¼Œçµæ•åº¦è¶Šé«˜
            stability = 1 / (result['std_foreground'] + 0.001)  # æ ‡å‡†å·®è¶Šå°ï¼Œç¨³å®šæ€§è¶Šå¥½
            speed = 1 / (result['avg_processing_time'] + 0.001)  # å¤„ç†æ—¶é—´è¶ŠçŸ­ï¼Œé€Ÿåº¦è¶Šå¿«
            light_response = result['avg_light_response']  # äº®åº¦å˜åŒ–å“åº”

            # å½’ä¸€åŒ–åˆ°0-1èŒƒå›´
            max_sensitivity = max([r['avg_foreground']
                                  for r in results.values()])
            max_stability = max([1/(r['std_foreground']+0.001)
                                for r in results.values()])
            max_speed = max([1/(r['avg_processing_time']+0.001)
                            for r in results.values()])
            max_light_response = max([r['avg_light_response']
                                     for r in results.values()])

            normalized_performance = [
                sensitivity / max_sensitivity,
                stability / max_stability,
                speed / max_speed,
                light_response / (max_light_response + 0.001)
            ]

            strategy_performance[strategy_name] = normalized_performance

        # ç»˜åˆ¶é›·è¾¾å›¾
        angles = np.linspace(0, 2*np.pi, len(metrics), endpoint=False).tolist()
        angles += angles[:1]  # é—­åˆå›¾å½¢

        for strategy_name, performance in strategy_performance.items():
            performance += performance[:1]  # é—­åˆå›¾å½¢
            ax4.plot(angles, performance, 'o-', label=strategy_name)
            ax4.fill(angles, performance, alpha=0.1)

        ax4.set_xticks(angles[:-1])
        ax4.set_xticklabels(metrics)
        ax4.set_title('strategy_performance_radar')
        ax4.legend(loc='upper right')

        plt.tight_layout()

        # ä¿å­˜å›¾åƒ
        output_dir = Path("outputs/experiments")
        output_dir.mkdir(parents=True, exist_ok=True)
        plt.savefig(output_dir / "light_change_strategy_comparison.png",
                    dpi=300, bbox_inches='tight')
        plt.show()

        print(f"ğŸ’¾ å¯è§†åŒ–ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")

    def generate_report(self, results, significant_changes):
        """ç”Ÿæˆå®éªŒæŠ¥å‘Š"""
        print("\nğŸ“„ ç”Ÿæˆå®éªŒæŠ¥å‘Š...")

        report = {
            "å®éªŒæ—¶é—´": time.strftime("%Y-%m-%d %H:%M:%S"),
            "æµ‹è¯•è§†é¢‘": self.video_path,
            "æ˜¾è‘—äº®åº¦å˜åŒ–æ¬¡æ•°": len(significant_changes),
            "ç­–ç•¥æ¯”è¾ƒç»“æœ": {}
        }

        # æ‰¾å‡ºæœ€ä½³ç­–ç•¥
        best_sensitivity = max(
            results.items(), key=lambda x: x[1]['avg_foreground'])
        best_stability = min(
            results.items(), key=lambda x: x[1]['std_foreground'])
        best_speed = min(
            results.items(), key=lambda x: x[1]['avg_processing_time'])
        best_light_response = max(
            results.items(), key=lambda x: x[1]['avg_light_response'])

        for strategy_name, result in results.items():
            report["ç­–ç•¥æ¯”è¾ƒç»“æœ"][strategy_name] = {
                "å¹³å‡å‰æ™¯æ¯”ä¾‹": f"{result['avg_foreground']:.4f}",
                "å‰æ™¯ç¨³å®šæ€§(æ ‡å‡†å·®)": f"{result['std_foreground']:.4f}",
                "å¹³å‡å¤„ç†æ—¶é—´(ms)": f"{result['avg_processing_time']*1000:.2f}",
                "äº®åº¦å˜åŒ–å“åº”": f"{result['avg_light_response']:.4f}"
            }

        report["æ¨èç­–ç•¥"] = {
            "æœ€é«˜çµæ•åº¦": best_sensitivity[0],
            "æœ€ç¨³å®š": best_stability[0],
            "æœ€å¿«é€Ÿåº¦": best_speed[0],
            "æœ€ä½³äº®åº¦å“åº”": best_light_response[0]
        }

        # ä¿å­˜æŠ¥å‘Š
        import json
        report_file = Path(
            "outputs/experiments/light_change_experiment_report.json")
        report_file.parent.mkdir(parents=True, exist_ok=True)

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        print(f"ğŸ’¾ å®éªŒæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

        # æ‰“å°æ‘˜è¦
        print("\n" + "=" * 60)
        print("ğŸ“‹ å®éªŒæŠ¥å‘Šæ‘˜è¦")
        print("=" * 60)
        print(f"æµ‹è¯•è§†é¢‘: {Path(self.video_path).name}")
        print(f"æ˜¾è‘—äº®åº¦å˜åŒ–: {len(significant_changes)} æ¬¡")
        print("\nç­–ç•¥è¡¨ç°æ’å:")
        print("1. æœ€é«˜çµæ•åº¦:", best_sensitivity[0])
        print("2. æœ€ç¨³å®š:", best_stability[0])
        print("3. æœ€å¿«é€Ÿåº¦:", best_speed[0])
        print("4. æœ€ä½³äº®åº¦å“åº”:", best_light_response[0])


def main():
    """ä¸»å®éªŒå‡½æ•°"""
    video_path = "data/test_videos/train_enter_station.mp4"

    if not Path(video_path).exists():
        print(f"âŒ è§†é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {video_path}")
        print("ğŸ’¡ è¯·ç¡®ä¿æµ‹è¯•è§†é¢‘æ–‡ä»¶å­˜åœ¨")
        return

    print("=" * 60)
    print("ğŸ¬ é“è½¨å…‰å½±å˜åŒ–ç­–ç•¥çœŸå®è§†é¢‘å®éªŒ")
    print("=" * 60)

    experiment = LightChangeExperiment(video_path)

    # è¿è¡Œå®éªŒ
    results, brightness_changes, significant_changes = experiment.run_strategy_comparison(
        max_frames=700)

    if results:
        # å¯è§†åŒ–ç»“æœ
        experiment.visualize_results(
            results, brightness_changes, significant_changes)

        # ç”ŸæˆæŠ¥å‘Š
        experiment.generate_report(results, significant_changes)

        print("\nğŸ‰ å®éªŒå®Œæˆï¼")
    else:
        print("âŒ å®éªŒå¤±è´¥ï¼Œæ— æœ‰æ•ˆç»“æœ")


if __name__ == "__main__":
    main()
